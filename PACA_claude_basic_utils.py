import json
# from langchain.chat_models import ChatOpenAI, ChatAnthropic
from langchain_anthropic import ChatAnthropic
# from langchain_openai import ChatOpenAI
# from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain.schema import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.chat_history import InMemoryChatMessageHistory
import streamlit as st
from SP_utils import create_conversational_agent, save_to_firebase
from firebase_config import get_firebase_ref
import time
import pandas as pd
import io


paca_llm_claude = ChatAnthropic(
    model="claude-haiku-4-5-20251001",
    temperature=0.7,
    streaming=True,
)

firebase_ref = get_firebase_ref()

basic_prompt = """
You are a psychiatrist conducting an initial interview with a new patient. Your goal is to gather relevant information about the patient's mental health, symptoms, and background. Ask open-ended questions and follow up on the patient's responses to gain a comprehensive understanding of their situation. When starting the conversation, begin with exactly these words: "안녕하세요, 저는 정신과 의사 김민수입니다. 이름이 어떻게 되시나요?". Proceed in Korean. Ask questions one at a time, at most.

IMPORTANT INSTRUCTIONS FOR NATURAL CONVERSATION:
- You must NEVER use bullet points, lists, headings, or any structured formatting of any kind.
- Keep your responses brief and conversational, like a real doctor talking to a patient.
- Never use bullet points, lists, or structured formatting in your responses.
- Ask one question at a time, waiting for the patient’s response before asking the next.
- Use natural spoken language with contractions and an informal tone.
- If the patient mentions something relevant, follow up naturally with a related question.
- Do not provide summaries, explanations, or meta-commentary unless asked.
- Keep each response to about 1-3 sentences per turn.

After the interview with the patient is complete, someone will come to ask you about the patient. As an experienced psychiatrist, use appropriate reasoning, your professional judgment, and the information you've gathered during the interview to answer their questions. If you cannot determine something even with appropriate reasoning and your expertise, respond with "I don't know".
"""


@st.cache_resource
def create_paca_agent(paca_version):
    system_prompt = basic_prompt

    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{human_input}")
    ])

    # Use InMemoryChatMessageHistory for proper message history management
    # This memory object will persist across Streamlit reruns because of @st.cache_resource
    memory = InMemoryChatMessageHistory()

    def paca_agent(human_input, is_initial_prompt=False):
        chain = chat_prompt | paca_llm_claude
        
        # Create a list to pass to the chain with current memory state
        messages = list(memory.messages) if memory.messages else []
        
        response = chain.invoke({
            "chat_history": messages,
            "human_input": human_input,
        })
        
        # Add messages to memory in the correct order
        if not is_initial_prompt:
            # For regular conversation, add user message first, then AI response
            memory.add_user_message(human_input)
            memory.add_ai_message(response.content)
        else:
            # For initial prompt, only add the AI response (the system is generating the initial message)
            memory.add_ai_message(response.content)
        
        return response.content

    return paca_agent, memory, paca_version


def simulate_conversation(paca_agent, sp_agent, max_turns=100):
    # Initial greeting from the doctor
    initial_prompt = "안녕하세요, 저는 정신과 의사 김민수입니다. 이름이 어떻게 되시나요?"
    
    # Don't call is_initial_prompt here - just add the message to memory directly
    # The initial prompt is NOT generated by LLM, it's a hardcoded greeting
    yield ("PACA", initial_prompt)

    current_speaker = "SP"
    current_message = initial_prompt

    for _ in range(max_turns):
        if current_speaker == "SP":
            response = sp_agent(current_message)
            yield ("SP", response)
            current_speaker = "PACA"
        else:
            # Now PACA responds to the patient's message
            response = paca_agent(current_message, is_initial_prompt=False)
            yield ("PACA", response)
            current_speaker = "SP"

        current_message = response


def save_conversation_to_csv(conversation):
    df = pd.DataFrame(conversation, columns=["Speaker", "Message"])
    paca_messages = df[df["Speaker"] == "PACA"]["Message"]
    sp_messages = df[df["Speaker"] == "SP"]["Message"]

    result_df = pd.DataFrame({
        "PACA": paca_messages.reset_index(drop=True),
        "SP": sp_messages.reset_index(drop=True)
    })

    csv_buffer = io.BytesIO()
    result_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
    csv_buffer.seek(0)

    return csv_buffer.getvalue()


def save_ai_conversation_to_firebase(firebase_ref, client_number, conversation, paca_version, sp_version):
    conversation_data = [
        {'speaker': speaker, 'message': message}
        for speaker, message in conversation
    ]

    timestamp = int(time.time())

    content = {
        'paca_version': paca_version,
        'sp_version': sp_version,
        'timestamp': timestamp,
        'data': conversation_data
    }

    conversation_id = f"ai_conversation_paca{paca_version}_sp{sp_version}_{timestamp}"
    save_to_firebase(
        firebase_ref, client_number, conversation_id, content)

    return conversation_id
