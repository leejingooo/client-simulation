"""
ë…¼ë¬¸ìš© Figure ìƒì„± íŽ˜ì´ì§€
Publication-quality figures for PSYCHE framework paper

ëª¨ë“  í°íŠ¸ëŠ” Helveticaë¡œ í†µì¼
All fonts unified to Helvetica
"""

import streamlit as st
import pandas as pd
import numpy as np
from firebase_config import get_firebase_ref
from expert_validation_utils import sanitize_firebase_key
import matplotlib.pyplot as plt
import matplotlib
from matplotlib import rcParams
from scipy import stats
import seaborn as sns
import io

# ================================
# Configuration
# ================================
st.set_page_config(
    page_title="Publication Figures",
    page_icon="ðŸ“Š",
    layout="wide"
)

# Helvetica í°íŠ¸ ì„¤ì •
rcParams['font.family'] = 'Helvetica'
rcParams['axes.unicode_minus'] = False

# Seaborn ìŠ¤íƒ€ì¼
sns.set_style("ticks")

# ================================
# PRESET - Experiment Numbers
# ================================
EXPERIMENT_NUMBERS = [
    # 6201 MDD
    (6201, 3111), (6201, 3117),  # gptsmaller
    (6201, 1121), (6201, 1123),  # gptlarge
    (6201, 3134), (6201, 3138),  # claudesmaller
    (6201, 1143), (6201, 1145),  # claudelarge
    # 6202 BD
    (6202, 3211), (6202, 3212),  # gptsmaller
    (6202, 1221), (6202, 1222),  # gptlarge
    (6202, 3231), (6202, 3234),  # claudesmaller
    (6202, 1241), (6202, 1242),  # claudelarge
    # 6206 OCD
    (6206, 3611), (6206, 3612),  # gptsmaller
    (6206, 1621), (6206, 1622),  # gptlarge
    (6206, 3631), (6206, 3632),  # claudesmaller
    (6206, 1641), (6206, 1642),  # claudelarge
]

VALIDATORS = ["ì´ê°•í† ", "ê¹€íƒœí™˜", "ê¹€ê´‘í˜„", "ê¹€ì£¼ì˜¤", "í—ˆìœ¨", "ìž¥ìž¬ìš©"]

VALIDATOR_INITIALS = {
    "ì´ê°•í† ": "A",
    "ê¹€íƒœí™˜": "B",
    "ê¹€ê´‘í˜„": "C",
    "ê¹€ì£¼ì˜¤": "D",
    "í—ˆìœ¨": "E",
    "ìž¥ìž¬ìš©": "F"
}

DISORDER_MAP = {6201: "mdd", 6202: "bd", 6206: "ocd"}
DISORDER_NAMES = {
    "mdd": "Major Depressive Disorder",
    "bd": "Bipolar Disorder",
    "ocd": "Obsessive-Compulsive Disorder"
}

MODEL_BY_EXP = {
    3111: 'gptsmaller', 3117: 'gptsmaller',
    1121: 'gptlarge', 1123: 'gptlarge',
    3134: 'claudesmaller', 3138: 'claudesmaller',
    1143: 'claudelarge', 1145: 'claudelarge',
    3211: 'gptsmaller', 3212: 'gptsmaller',
    1221: 'gptlarge', 1222: 'gptlarge',
    3231: 'claudesmaller', 3234: 'claudesmaller',
    1241: 'claudelarge', 1242: 'claudelarge',
    3611: 'gptsmaller', 3612: 'gptsmaller',
    1621: 'gptlarge', 1622: 'gptlarge',
    3631: 'claudesmaller', 3632: 'claudesmaller',
    1641: 'claudelarge', 1642: 'claudelarge',
}

# ìƒ‰ìƒ ë° ë§ˆì»¤ ë§¤í•‘ (ë…¼ë¬¸ìš©)
COLOR_MAP = {
    "gptsmaller": "#2ecc71",     # ì´ˆë¡ìƒ‰
    "gptlarge": "#27ae60",       # ì§„í•œ ì´ˆë¡ìƒ‰
    "claudesmaller": "#e67e22",  # ì£¼í™©ìƒ‰
    "claudelarge": "#d35400"     # ì§„í•œ ì£¼í™©ìƒ‰
}

MARKER_MAP = {
    "gptsmaller": {"marker": "o", "size": 300},
    "gptlarge": {"marker": "*", "size": 600},
    "claudesmaller": {"marker": "o", "size": 300},
    "claudelarge": {"marker": "*", "size": 600}
}

LABEL_MAP = {
    "gptsmaller": "GPT-Small",
    "gptlarge": "GPT-Large",
    "claudesmaller": "Claude-Small",
    "claudelarge": "Claude-Large"
}

def get_model_from_exp(exp_num):
    """Identify model from experiment number."""
    return MODEL_BY_EXP.get(exp_num, 'unknown')

# ================================
# Data Loading Functions
# ================================
def load_expert_scores(root_data):
    """Load expert scores for all validators and experiments."""
    expert_data = {}
    for validator in VALIDATORS:
        sanitized_name = sanitize_firebase_key(validator)
        validator_scores = {}
        for client_num, exp_num in EXPERIMENT_NUMBERS:
            key = f"expert_{sanitized_name}_{client_num}_{exp_num}"
            data = (root_data or {}).get(key, {}) or {}
            if 'expert_score' in data:
                validator_scores[(client_num, exp_num)] = data['expert_score']
            elif 'psyche_score' in data:
                validator_scores[(client_num, exp_num)] = data['psyche_score']
            else:
                validator_scores[(client_num, exp_num)] = None
        expert_data[validator] = validator_scores
    return expert_data

def load_psyche_scores(root_data):
    """Load automated PSYCHE scores."""
    psyche_data = {}
    for client_num, exp_num in EXPERIMENT_NUMBERS:
        value = None
        target_prefix = f"clients_{client_num}_psyche_"
        target_suffix = f"_{exp_num}"
        for key, data in (root_data or {}).items():
            if not key.startswith(target_prefix):
                continue
            if not key.endswith(target_suffix):
                continue
            record = data or {}
            if 'psyche_score' in record:
                value = record['psyche_score']
                break
        psyche_data[(client_num, exp_num)] = value
    return psyche_data

def calculate_average_expert_scores(expert_data):
    """Calculate average expert scores across validators."""
    avg_scores = {}
    for exp in EXPERIMENT_NUMBERS:
        scores = [expert_data[v].get(exp) for v in VALIDATORS if expert_data[v].get(exp) is not None]
        avg_scores[exp] = np.mean(scores) if scores else None
    return avg_scores

# ================================
# Figure 1: PSYCHE-Expert Correlation
# ================================
def create_correlation_plot_average(psyche_scores, avg_expert_scores, figsize=(8, 8)):
    """Figure 1-1: Average expert score correlation plot."""
    fig, ax = plt.subplots(figsize=figsize)
    
    # ë°ì´í„° ì¤€ë¹„
    data_by_model = {model: [] for model in COLOR_MAP.keys()}
    
    for exp in EXPERIMENT_NUMBERS:
        psyche = psyche_scores.get(exp)
        expert = avg_expert_scores.get(exp)
        if psyche is not None and expert is not None:
            model = get_model_from_exp(exp[1])
            if model in data_by_model:
                data_by_model[model].append((psyche, expert))
    
    # Scatter plot
    all_x, all_y = [], []
    for model, points in data_by_model.items():
        if points:
            x, y = zip(*points)
            all_x.extend(x)
            all_y.extend(y)
            ax.scatter(x, y, 
                      c=COLOR_MAP[model],
                      marker=MARKER_MAP[model]["marker"],
                      s=MARKER_MAP[model]["size"],
                      label=LABEL_MAP[model],
                      alpha=0.7)
    
    # íšŒê·€ì„ 
    if len(all_x) >= 2:
        z = np.polyfit(all_x, all_y, 1)
        p = np.poly1d(z)
        x_line = np.linspace(min(all_x), max(all_x), 100)
        ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
        
        # Correlation
        correlation, p_value = stats.pearsonr(all_x, all_y)
        p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
        ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
               transform=ax.transAxes, fontsize=22, family='Helvetica')
    
    # ìŠ¤íƒ€ì¼ë§
    ax.set_title('PSYCHE SCORE vs. Expert score', fontsize=36, pad=20, family='Helvetica')
    ax.set_xlabel('PSYCHE SCORE', fontsize=36, family='Helvetica')
    ax.set_ylabel('Expert score', fontsize=36, family='Helvetica')
    ax.set_yticks([5, 35, 65])
    ax.set_xticks([5, 30, 55])
    ax.tick_params(labelsize=32)
    ax.legend(loc='upper left', prop={'size': 18, 'weight': 'bold', 'family': 'Helvetica'})
    
    # í…Œë‘ë¦¬
    for spine in ax.spines.values():
        spine.set_color('black')
        spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig

def create_correlation_plot_average_with_errors(psyche_scores, avg_expert_scores, figsize=(8, 8)):
    """Figure 1-1b: Average expert score correlation plot with top 3 error highlighting."""
    fig, ax = plt.subplots(figsize=figsize)
    
    # ë°ì´í„° ì¤€ë¹„ ë° error ê³„ì‚°
    data_by_model = {model: [] for model in COLOR_MAP.keys()}
    errors = []  # List of (abs_error, error, exp, psyche, expert, model)
    
    for exp in EXPERIMENT_NUMBERS:
        psyche = psyche_scores.get(exp)
        expert = avg_expert_scores.get(exp)
        if psyche is not None and expert is not None:
            model = get_model_from_exp(exp[1])
            data_by_model[model].append((psyche, expert))
            
            # Calculate error (Expert - PSYCHE)
            error = expert - psyche
            abs_error = abs(error)
            errors.append((abs_error, error, exp, psyche, expert, model))
    
    # Sort by absolute error and get top 3
    errors.sort(reverse=True, key=lambda x: x[0])
    top_3_errors = errors[:3]
    top_3_exps = {err[2] for err in top_3_errors}
    
    # Scatter plot - regular points
    all_x, all_y = [], []
    for model, points in data_by_model.items():
        if points:
            x_vals = [p[0] for p in points]
            y_vals = [p[1] for p in points]
            ax.scatter(x_vals, y_vals, 
                      color=COLOR_MAP[model],
                      label=LABEL_MAP[model],
                      s=MARKER_MAP[model]['size'],
                      marker=MARKER_MAP[model]['marker'],
                      alpha=0.7,
                      edgecolors='black',
                      linewidths=1.5,
                      zorder=2)
            all_x.extend(x_vals)
            all_y.extend(y_vals)
    
    # Highlight top 3 errors with red circles and labels
    for rank, (abs_err, err, exp, psyche, expert, model) in enumerate(top_3_errors, 1):
        # Large red circle
        ax.scatter([psyche], [expert], 
                  s=800, 
                  facecolors='none',
                  edgecolors='red',
                  linewidths=3,
                  zorder=3)
        
        # Rank label
        ax.text(psyche, expert, str(rank),
               fontsize=28, fontweight='bold',
               ha='center', va='center',
               color='red',
               family='Helvetica',
               zorder=4)
    
    # íšŒê·€ì„ 
    if len(all_x) >= 2:
        z = np.polyfit(all_x, all_y, 1)
        p = np.poly1d(z)
        x_line = np.linspace(min(all_x), max(all_x), 100)
        ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2, zorder=1)
        
        # Correlation
        correlation, p_value = stats.pearsonr(all_x, all_y)
        p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
        ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
               transform=ax.transAxes, fontsize=22, family='Helvetica')
    
    # ìŠ¤íƒ€ì¼ë§
    ax.set_title('PSYCHE SCORE vs. Expert score\n(Top 3 Errors Highlighted)', 
                fontsize=32, pad=20, family='Helvetica')
    ax.set_xlabel('PSYCHE SCORE', fontsize=36, family='Helvetica')
    ax.set_ylabel('Expert score', fontsize=36, family='Helvetica')
    ax.set_yticks([5, 35, 65])
    ax.set_xticks([5, 30, 55])
    ax.tick_params(labelsize=32)
    ax.legend(loc='upper left', prop={'size': 18, 'weight': 'bold', 'family': 'Helvetica'})
    
    # í…Œë‘ë¦¬
    for spine in ax.spines.values():
        spine.set_color('black')
        spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig, top_3_errors

def create_correlation_plot_average_with_residuals(psyche_scores, avg_expert_scores, figsize=(8, 8)):
    """Figure 1-1c: Average expert score correlation plot with top 3 residual errors highlighted.
    
    Residual = distance from regression line (more statistically meaningful outlier detection)
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    # ë°ì´í„° ì¤€ë¹„
    data_by_model = {model: [] for model in COLOR_MAP.keys()}
    all_data = []  # (psyche, expert, exp, model)
    
    for exp in EXPERIMENT_NUMBERS:
        psyche = psyche_scores.get(exp)
        expert = avg_expert_scores.get(exp)
        if psyche is not None and expert is not None:
            model = get_model_from_exp(exp[1])
            data_by_model[model].append((psyche, expert))
            all_data.append((psyche, expert, exp, model))
    
    # íšŒê·€ì„  ê³„ì‚°
    all_x = [d[0] for d in all_data]
    all_y = [d[1] for d in all_data]
    
    if len(all_x) < 2:
        return None, []
    
    # Linear regression: y = ax + b
    z = np.polyfit(all_x, all_y, 1)
    p = np.poly1d(z)
    
    # Calculate residuals for each point
    residuals = []  # (abs_residual, residual, exp, psyche, expert, model, predicted)
    for psyche, expert, exp, model in all_data:
        predicted = p(psyche)
        residual = expert - predicted  # Actual - Predicted
        abs_residual = abs(residual)
        residuals.append((abs_residual, residual, exp, psyche, expert, model, predicted))
    
    # Sort by absolute residual and get top 3
    residuals.sort(reverse=True, key=lambda x: x[0])
    top_3_residuals = residuals[:3]
    top_3_exps = {res[2] for res in top_3_residuals}
    
    # Scatter plot - regular points
    for model, points in data_by_model.items():
        if points:
            x_vals = [p[0] for p in points]
            y_vals = [p[1] for p in points]
            ax.scatter(x_vals, y_vals, 
                      color=COLOR_MAP[model],
                      label=LABEL_MAP[model],
                      s=MARKER_MAP[model]['size'],
                      marker=MARKER_MAP[model]['marker'],
                      alpha=0.7,
                      edgecolors='black',
                      linewidths=1.5,
                      zorder=2)
    
    # íšŒê·€ì„ 
    x_line = np.linspace(min(all_x), max(all_x), 100)
    ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2, zorder=1)
    
    # Highlight top 3 residuals with red circles and labels
    for rank, (abs_res, res, exp, psyche, expert, model, predicted) in enumerate(top_3_residuals, 1):
        # Large red circle
        ax.scatter([psyche], [expert], 
                  s=800, 
                  facecolors='none',
                  edgecolors='red',
                  linewidths=3,
                  zorder=3)
        
        # Rank label
        ax.text(psyche, expert, str(rank),
               fontsize=28, fontweight='bold',
               ha='center', va='center',
               color='red',
               family='Helvetica',
               zorder=4)
        
        # Draw residual line (vertical line to regression)
        ax.plot([psyche, psyche], [expert, predicted],
               color='red', linestyle='--', linewidth=2, alpha=0.6, zorder=1)
    
    # Correlation info
    correlation, p_value = stats.pearsonr(all_x, all_y)
    p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
    ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
           transform=ax.transAxes, fontsize=22, family='Helvetica')
    
    # ìŠ¤íƒ€ì¼ë§
    ax.set_title('PSYCHE SCORE vs. Expert score\n(Top 3 Residual Errors Highlighted)', 
                fontsize=32, pad=20, family='Helvetica')
    ax.set_xlabel('PSYCHE SCORE', fontsize=36, family='Helvetica')
    ax.set_ylabel('Expert score', fontsize=36, family='Helvetica')
    ax.set_yticks([5, 35, 65])
    ax.set_xticks([5, 30, 55])
    ax.tick_params(labelsize=32)
    ax.legend(loc='upper left', prop={'size': 18, 'weight': 'bold', 'family': 'Helvetica'})
    
    # í…Œë‘ë¦¬
    for spine in ax.spines.values():
        spine.set_color('black')
        spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig, top_3_residuals

def create_correlation_plot_by_validator(psyche_scores, expert_data):
    """Figure 1-2: Individual validator correlation plots."""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for idx, validator in enumerate(VALIDATORS):
        ax = axes[idx]
        
        # ë°ì´í„° ìˆ˜ì§‘
        validator_x, validator_y = [], []
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        
        for exp in EXPERIMENT_NUMBERS:
            psyche = psyche_scores.get(exp)
            expert = expert_data[validator].get(exp)
            if psyche is not None and expert is not None:
                validator_x.append(psyche)
                validator_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„  ë° correlation
        if len(validator_x) >= 2:
            z = np.polyfit(validator_x, validator_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(validator_x), max(validator_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(validator_x, validator_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=18, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(VALIDATOR_INITIALS[validator], fontsize=24, fontweight='bold', family='Helvetica')
        ax.set_xlabel('PSYCHE SCORE', fontsize=22, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=22, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=20)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig

def create_correlation_plot_by_disorder(psyche_scores, avg_expert_scores):
    """Figure 1-3: Disorder-specific correlation plots."""
    fig, axes = plt.subplots(1, 3, figsize=(24, 8))
    
    for idx, (disorder_code, disorder_name) in enumerate([(6201, "MDD"), (6202, "BD"), (6206, "OCD")]):
        ax = axes[idx]
        
        # í•´ë‹¹ disorder ë°ì´í„°ë§Œ í•„í„°ë§
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp[0] != disorder_code:
                continue
            psyche = psyche_scores.get(exp)
            expert = avg_expert_scores.get(exp)
            if psyche is not None and expert is not None:
                all_x.append(psyche)
                all_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=24, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§ (1-1ê³¼ ë™ì¼)
        ax.set_title(DISORDER_NAMES[DISORDER_MAP[disorder_code]], fontsize=36, fontweight='bold', family='Helvetica', pad=20)
        ax.set_xlabel('PSYCHE SCORE', fontsize=36, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=36, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=32)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig

def calculate_category_scores(element_scores_psyche, element_scores_expert_dict):
    """Calculate category-level scores (Subjective, Impulsivity, Behavior) for correlation analysis.
    
    Returns:
    - psyche_category_scores: {(client_num, exp_num): {'Subjective': float, 'Impulsivity': float, 'Behavior': float}}
    - expert_category_scores: {validator: {(client_num, exp_num): {'Subjective': float, 'Impulsivity': float, 'Behavior': float}}}
    """
    from evaluator import PSYCHE_RUBRIC
    
    # Categoryë³„ element ë¶„ë¥˜
    impulsivity_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') == 'impulsivity']
    behavior_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') == 'behavior']
    subjective_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') in ['g-eval', 'binary'] and v.get('weight') == 1]
    
    # Weights
    weight_subjective = 1
    weight_impulsivity = 5
    weight_behavior = 2
    
    # PSYCHE category scores
    psyche_category_scores = {}
    for exp in EXPERIMENT_NUMBERS:
        if exp not in element_scores_psyche:
            continue
        
        psyche_elements = element_scores_psyche[exp]
        categories = {
            'Subjective': 0,
            'Impulsivity': 0,
            'Behavior': 0
        }
        
        # Subjective
        for elem in subjective_elements:
            if elem in psyche_elements and 'score' in psyche_elements[elem]:
                categories['Subjective'] += psyche_elements[elem]['score'] * weight_subjective
        
        # Impulsivity
        for elem in impulsivity_elements:
            if elem in psyche_elements and 'score' in psyche_elements[elem]:
                categories['Impulsivity'] += psyche_elements[elem]['score'] * weight_impulsivity
        
        # Behavior
        for elem in behavior_elements:
            if elem in psyche_elements and 'score' in psyche_elements[elem]:
                categories['Behavior'] += psyche_elements[elem]['score'] * weight_behavior
        
        psyche_category_scores[exp] = categories
    
    # Expert category scores (averaged across validators)
    expert_category_scores = {validator: {} for validator in VALIDATORS}
    
    for validator in VALIDATORS:
        for exp in EXPERIMENT_NUMBERS:
            if exp not in element_scores_expert_dict[validator]:
                continue
            
            expert_elements = element_scores_expert_dict[validator][exp]
            categories = {
                'Subjective': 0,
                'Impulsivity': 0,
                'Behavior': 0
            }
            
            # Subjective
            for elem in subjective_elements:
                if elem in expert_elements:
                    elem_data = expert_elements[elem]
                    if isinstance(elem_data, dict) and 'weighted_score' in elem_data:
                        categories['Subjective'] += elem_data['weighted_score']
                    elif isinstance(elem_data, dict) and 'score' in elem_data:
                        categories['Subjective'] += elem_data['score'] * weight_subjective
            
            # Impulsivity
            for elem in impulsivity_elements:
                if elem in expert_elements:
                    elem_data = expert_elements[elem]
                    if isinstance(elem_data, dict) and 'weighted_score' in elem_data:
                        categories['Impulsivity'] += elem_data['weighted_score']
                    elif isinstance(elem_data, dict) and 'score' in elem_data:
                        categories['Impulsivity'] += elem_data['score'] * weight_impulsivity
            
            # Behavior
            for elem in behavior_elements:
                if elem in expert_elements:
                    elem_data = expert_elements[elem]
                    if isinstance(elem_data, dict) and 'weighted_score' in elem_data:
                        categories['Behavior'] += elem_data['weighted_score']
                    elif isinstance(elem_data, dict) and 'score' in elem_data:
                        categories['Behavior'] += elem_data['score'] * weight_behavior
            
            expert_category_scores[validator][exp] = categories
    
    return psyche_category_scores, expert_category_scores

def create_correlation_plot_by_category(psyche_category_scores, expert_category_scores):
    """Figure 1-4: Category-level correlation analysis (Subjective, Impulsivity, Behavior)."""
    fig, axes = plt.subplots(1, 3, figsize=(24, 8))
    
    categories = ['Subjective', 'Impulsivity', 'Behavior']
    category_labels = {
        'Subjective': 'Subjective Information',
        'Impulsivity': 'Impulsivity',
        'Behavior': 'MFC-Behavior'
    }
    
    for idx, category in enumerate(categories):
        ax = axes[idx]
        
        # ë°ì´í„° ìˆ˜ì§‘ - validatorë³„ í‰ê· 
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp not in psyche_category_scores:
                continue
            
            psyche_score = psyche_category_scores[exp][category]
            
            # Expert score - average across all validators
            expert_scores = []
            for validator in VALIDATORS:
                if exp in expert_category_scores[validator]:
                    expert_scores.append(expert_category_scores[validator][exp][category])
            
            if not expert_scores:
                continue
            
            expert_score = np.mean(expert_scores)
            model = get_model_from_exp(exp[1])
            
            data_by_model[model].append((psyche_score, expert_score))
            all_x.append(psyche_score)
            all_y.append(expert_score)
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x_vals = [p[0] for p in points]
                y_vals = [p[1] for p in points]
                ax.scatter(x_vals, y_vals, 
                          color=COLOR_MAP[model],
                          label=LABEL_MAP[model],
                          s=MARKER_MAP[model]['size'],
                          marker=MARKER_MAP[model]['marker'],
                          alpha=0.7,
                          edgecolors='black',
                          linewidths=1.5)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            # Correlation
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=24, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(category_labels[category], fontsize=36, fontweight='bold', family='Helvetica', pad=20)
        ax.set_xlabel('PSYCHE SCORE', fontsize=36, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=36, family='Helvetica')
        ax.tick_params(labelsize=32)
        ax.grid(False)
        
        # # Legend only on first subplot
        # if idx == 0:
        #     ax.legend(loc='lower right', prop={'size': 18, 'weight': 'bold', 'family': 'Helvetica'})
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    plt.tight_layout()
    return fig

def create_combined_correlation_figure(psyche_scores, avg_expert_scores, expert_data, psyche_category_scores, expert_category_scores):
    """Combined Figure: Validator-specific, Disease-specific, and Category-specific correlation plots.
    
    Layout:
    - Rows 0-1: Validator-specific (2x3 grid) - (a)
    - Row 2: Disease-specific (1x3 grid) - (b)
    - Row 3: Category-specific (1x3 grid) - (c)
    """
    fig = plt.figure(figsize=(24, 32))
    
    # GridSpec with spacing between sections
    gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3,
                          height_ratios=[1, 1, 1.2, 1.2])
    
    # ========================================
    # (a) Validator-specific: Rows 0-1 (2x3)
    # ========================================
    for idx, validator in enumerate(VALIDATORS):
        row = idx // 3
        col = idx % 3
        ax = fig.add_subplot(gs[row, col])
        
        # ë°ì´í„° ìˆ˜ì§‘
        validator_x, validator_y = [], []
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        
        for exp in EXPERIMENT_NUMBERS:
            psyche = psyche_scores.get(exp)
            expert = expert_data[validator].get(exp)
            if psyche is not None and expert is not None:
                validator_x.append(psyche)
                validator_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„ 
        if len(validator_x) >= 2:
            z = np.polyfit(validator_x, validator_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(validator_x), max(validator_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(validator_x, validator_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=18, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(VALIDATOR_INITIALS[validator], fontsize=24, fontweight='bold', family='Helvetica')
        ax.set_xlabel('PSYCHE SCORE', fontsize=22, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=22, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=20)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (a) ë§ˆì»¤ ì¶”ê°€ - ì™¼ìª½ ìƒë‹¨
    fig.text(0.02, 0.95, '(a)', fontsize=32, fontweight='bold', family='Helvetica')
    
    # ========================================
    # (b) Disease-specific: Row 2 (1x3)
    # ========================================
    for idx, (disorder_code, disorder_name) in enumerate([(6201, "MDD"), (6202, "BD"), (6206, "OCD")]):
        ax = fig.add_subplot(gs[2, idx])
        
        # ë°ì´í„° í•„í„°ë§
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp[0] != disorder_code:
                continue
            psyche = psyche_scores.get(exp)
            expert = avg_expert_scores.get(exp)
            if psyche is not None and expert is not None:
                all_x.append(psyche)
                all_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=24, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(DISORDER_NAMES[DISORDER_MAP[disorder_code]], fontsize=30, fontweight='bold', family='Helvetica', pad=20)
        ax.set_xlabel('PSYCHE SCORE', fontsize=28, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=28, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=26)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (b) ë§ˆì»¤ ì¶”ê°€
    fig.text(0.02, 0.48, '(b)', fontsize=32, fontweight='bold', family='Helvetica')
    
    # ========================================
    # (c) Category-specific: Row 3 (1x3)
    # ========================================
    categories = ['Subjective', 'Impulsivity', 'Behavior']
    category_labels = {
        'Subjective': 'Subjective Information',
        'Impulsivity': 'Impulsivity',
        'Behavior': 'MFC-Behavior'
    }
    
    for idx, category in enumerate(categories):
        ax = fig.add_subplot(gs[3, idx])
        
        # ë°ì´í„° ìˆ˜ì§‘
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp not in psyche_category_scores:
                continue
            
            psyche_score = psyche_category_scores[exp][category]
            
            # Expert score - average across validators
            expert_scores = []
            for validator in VALIDATORS:
                if exp in expert_category_scores[validator]:
                    expert_scores.append(expert_category_scores[validator][exp][category])
            
            if not expert_scores:
                continue
            
            expert_score = np.mean(expert_scores)
            model = get_model_from_exp(exp[1])
            
            data_by_model[model].append((psyche_score, expert_score))
            all_x.append(psyche_score)
            all_y.append(expert_score)
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x_vals = [p[0] for p in points]
                y_vals = [p[1] for p in points]
                ax.scatter(x_vals, y_vals, 
                          color=COLOR_MAP[model],
                          label=LABEL_MAP[model],
                          s=MARKER_MAP[model]['size'],
                          marker=MARKER_MAP[model]['marker'],
                          alpha=0.7,
                          edgecolors='black',
                          linewidths=1.5)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=24, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(category_labels[category], fontsize=30, fontweight='bold', family='Helvetica', pad=20)
        ax.set_xlabel('PSYCHE SCORE', fontsize=28, family='Helvetica')
        ax.set_ylabel('Expert score', fontsize=28, family='Helvetica')
        ax.tick_params(labelsize=26)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (c) ë§ˆì»¤ ì¶”ê°€
    fig.text(0.02, 0.23, '(c)', fontsize=32, fontweight='bold', family='Helvetica')
    
    return fig

def create_combined_correlation_figure_v2(psyche_scores, avg_expert_scores, expert_data, psyche_category_scores, expert_category_scores):
    """Combined Figure Version 2: Validator-specific, Disease-specific, and Category-specific correlation plots.
    
    Alternative layout or styling for comparison testing.
    
    Layout:
    - Rows 0-1: Validator-specific (2x3 grid) - (a)
    - Row 2: Disease-specific (1x3 grid) - (b)
    - Row 3: Category-specific (1x3 grid) - (c)
    """
    fig = plt.figure(figsize=(24, 32))
    
    # GridSpec with spacing between sections
    gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3,
                          height_ratios=[1, 1, 1.2, 1.2])
    
    # ========================================
    # (a) Validator-specific: Rows 0-1 (2x3)
    # ========================================
    for idx, validator in enumerate(VALIDATORS):
        row = idx // 3
        col = idx % 3
        ax = fig.add_subplot(gs[row, col])
        
        # ë°ì´í„° ìˆ˜ì§‘
        validator_x, validator_y = [], []
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        
        for exp in EXPERIMENT_NUMBERS:
            psyche = psyche_scores.get(exp)
            expert = expert_data[validator].get(exp)
            if psyche is not None and expert is not None:
                validator_x.append(psyche)
                validator_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„ 
        if len(validator_x) >= 2:
            z = np.polyfit(validator_x, validator_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(validator_x), max(validator_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(validator_x, validator_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=22, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(VALIDATOR_INITIALS[validator], fontsize=30, fontweight='bold', family='Helvetica')
        # xlabelì€ ì•„ëž˜ìª½ í–‰(row==1)ë§Œ í‘œì‹œ
        if row == 1:
            ax.set_xlabel('PSYCHE SCORE', fontsize=28, family='Helvetica')
        # ylabelì€ ì™¼ìª½ ì—´(col==0)ë§Œ í‘œì‹œ
        if col == 0:
            ax.set_ylabel('Expert score', fontsize=28, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=26)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (a) ë§ˆì»¤ ì¶”ê°€ - ì™¼ìª½ ìƒë‹¨
    fig.text(0.02, 0.95, '(a)', fontsize=32, fontweight='bold', family='Helvetica')
    
    # ========================================
    # (b) Disease-specific: Row 2 (1x3)
    # ========================================
    for idx, (disorder_code, disorder_name) in enumerate([(6201, "MDD"), (6202, "BD"), (6206, "OCD")]):
        ax = fig.add_subplot(gs[2, idx])
        
        # ë°ì´í„° í•„í„°ë§
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp[0] != disorder_code:
                continue
            psyche = psyche_scores.get(exp)
            expert = avg_expert_scores.get(exp)
            if psyche is not None and expert is not None:
                all_x.append(psyche)
                all_y.append(expert)
                model = get_model_from_exp(exp[1])
                if model in data_by_model:
                    data_by_model[model].append((psyche, expert))
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x, y = zip(*points)
                ax.scatter(x, y,
                          c=COLOR_MAP[model],
                          marker=MARKER_MAP[model]["marker"],
                          s=MARKER_MAP[model]["size"],
                          alpha=0.7)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=22, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(DISORDER_NAMES[DISORDER_MAP[disorder_code]], fontsize=30, fontweight='bold', family='Helvetica', pad=20)
        # xlabelì€ ëª¨ë‘ í‘œì‹œ (ì•„ëž˜ìª½ ê°€ìž¥ìžë¦¬)
        ax.set_xlabel('PSYCHE SCORE', fontsize=28, family='Helvetica')
        # ylabelì€ ì™¼ìª½ ì²« ë²ˆì§¸ë§Œ í‘œì‹œ
        if idx == 0:
            ax.set_ylabel('Expert score', fontsize=28, family='Helvetica')
        ax.set_yticks([5, 35, 65])
        ax.set_xticks([5, 30, 55])
        ax.tick_params(labelsize=26)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (b) ë§ˆì»¤ ì¶”ê°€
    fig.text(0.02, 0.48, '(b)', fontsize=32, fontweight='bold', family='Helvetica')
    
    # ========================================
    # (c) Category-specific: Row 3 (1x3)
    # ========================================
    categories = ['Subjective', 'Impulsivity', 'Behavior']
    category_labels = {
        'Subjective': 'Subjective Information',
        'Impulsivity': 'Impulsivity',
        'Behavior': 'MFC-Behavior'
    }
    
    for idx, category in enumerate(categories):
        ax = fig.add_subplot(gs[3, idx])
        
        # ë°ì´í„° ìˆ˜ì§‘
        data_by_model = {model: [] for model in COLOR_MAP.keys()}
        all_x, all_y = [], []
        
        for exp in EXPERIMENT_NUMBERS:
            if exp not in psyche_category_scores:
                continue
            
            psyche_score = psyche_category_scores[exp][category]
            
            # Expert score - average across validators
            expert_scores = []
            for validator in VALIDATORS:
                if exp in expert_category_scores[validator]:
                    expert_scores.append(expert_category_scores[validator][exp][category])
            
            if not expert_scores:
                continue
            
            expert_score = np.mean(expert_scores)
            model = get_model_from_exp(exp[1])
            
            data_by_model[model].append((psyche_score, expert_score))
            all_x.append(psyche_score)
            all_y.append(expert_score)
        
        # Scatter plot
        for model, points in data_by_model.items():
            if points:
                x_vals = [p[0] for p in points]
                y_vals = [p[1] for p in points]
                ax.scatter(x_vals, y_vals, 
                          color=COLOR_MAP[model],
                          label=LABEL_MAP[model],
                          s=MARKER_MAP[model]['size'],
                          marker=MARKER_MAP[model]['marker'],
                          alpha=0.7,
                          edgecolors='black',
                          linewidths=1.5)
        
        # íšŒê·€ì„ 
        if len(all_x) >= 2:
            z = np.polyfit(all_x, all_y, 1)
            p = np.poly1d(z)
            x_line = np.linspace(min(all_x), max(all_x), 100)
            ax.plot(x_line, p(x_line), '#3498db', linestyle='-', linewidth=2)
            
            correlation, p_value = stats.pearsonr(all_x, all_y)
            p_text = 'p < 0.0001' if p_value < 0.0001 else f'p = {p_value:.4f}'
            ax.text(0.3, 0.10, f'r = {correlation:.4f}, {p_text}',
                   transform=ax.transAxes, fontsize=22, family='Helvetica')
        
        # ìŠ¤íƒ€ì¼ë§
        ax.set_title(category_labels[category], fontsize=30, fontweight='bold', family='Helvetica', pad=20)
        # xlabelì€ ëª¨ë‘ í‘œì‹œ (ì•„ëž˜ìª½ ê°€ìž¥ìžë¦¬)
        ax.set_xlabel('PSYCHE SCORE', fontsize=28, family='Helvetica')
        # ylabelì€ ì™¼ìª½ ì²« ë²ˆì§¸ë§Œ í‘œì‹œ
        if idx == 0:
            ax.set_ylabel('Expert score', fontsize=28, family='Helvetica')
        ax.tick_params(labelsize=26)
        ax.grid(False)
        
        for spine in ax.spines.values():
            spine.set_color('black')
            spine.set_linewidth(2)
    
    # (c) ë§ˆì»¤ ì¶”ê°€
    fig.text(0.02, 0.23, '(c)', fontsize=32, fontweight='bold', family='Helvetica')
    
    return fig

# ================================
# Figure 2: Weight-Correlation Analysis
# ================================
def load_element_scores(root_data):
    """Load element-level scores for weight analysis.
    
    Returns:
    - psyche_element_scores: {(client_num, exp_num): {element_name: {score: float}}}
    - expert_element_scores: {validator_name: {(client_num, exp_num): {element_name: {score: float}}}}
    """
    from evaluator import PSYCHE_RUBRIC
    
    psyche_element_scores = {}
    expert_element_scores = {validator: {} for validator in VALIDATORS}
    
    # Get valid element names from PSYCHE_RUBRIC
    valid_elements = set(PSYCHE_RUBRIC.keys())
    
    # Debug: collect all matching keys
    psyche_keys_found = []
    psyche_data_structure_sample = None
    
    # Load PSYCHE element scores
    for client_num, exp_num in EXPERIMENT_NUMBERS:
        target_prefix = f"clients_{client_num}_psyche_"
        target_suffix = f"_{exp_num}"
        
        for key, data in (root_data or {}).items():
            if not key.startswith(target_prefix):
                continue
            if not key.endswith(target_suffix):
                continue
            
            psyche_keys_found.append(key)
            record = data or {}
            
            # Save sample structure for debugging
            if psyche_data_structure_sample is None:
                psyche_data_structure_sample = {
                    'key': key,
                    'top_level_keys': list(record.keys())[:10],
                    'has_elements': 'elements' in record
                }
            
            # Try 'elements' layer first (standard structure)
            element_data = {}
            if 'elements' in record and isinstance(record['elements'], dict):
                for field_name, field_value in record['elements'].items():
                    if field_name in valid_elements:
                        element_data[field_name] = field_value
            else:
                # Fallback: extract element scores directly from record
                for field_name, field_value in record.items():
                    if field_name in valid_elements and isinstance(field_value, dict):
                        element_data[field_name] = field_value
            
            if element_data:
                psyche_element_scores[(client_num, exp_num)] = element_data
            break
    
    # Store debug info
    psyche_element_scores['_debug_keys'] = psyche_keys_found
    psyche_element_scores['_debug_structure'] = psyche_data_structure_sample
    
    # Load Expert element scores
    for validator in VALIDATORS:
        sanitized_name = sanitize_firebase_key(validator)
        for client_num, exp_num in EXPERIMENT_NUMBERS:
            key = f"expert_{sanitized_name}_{client_num}_{exp_num}"
            data = (root_data or {}).get(key, {}) or {}
            if 'elements' in data:
                # Filter to valid elements only
                valid_elem_data = {k: v for k, v in data['elements'].items() if k in valid_elements}
                if valid_elem_data:
                    expert_element_scores[validator][(client_num, exp_num)] = valid_elem_data
    
    return psyche_element_scores, expert_element_scores

def calculate_weighted_correlation_from_elements(element_scores_psyche, element_scores_expert_dict, 
                                                  weight_impulsivity, weight_behavior, weight_subjective=1,
                                                  expert_fixed_weights=None):
    """
    Elementë³„ ê°€ì¤‘ì¹˜ë¥¼ ë³€ê²½í•˜ì—¬ correlation ìž¬ê³„ì‚°
    
    Parameters:
    - element_scores_psyche: {(client_num, exp_num): {element_name: {score: float}}}
    - element_scores_expert_dict: {validator_name: {(client_num, exp_num): {element_name: {score: float}}}}
    - weight_impulsivity: Impulsivity category weight (default: 5)
    - weight_behavior: Behavior category weight (default: 2)
    - weight_subjective: Subjective category weight (fixed: 1)
    - expert_fixed_weights: (imp, beh, subj) tuple for fixed expert weights, or None for variable
    """
    from evaluator import PSYCHE_RUBRIC
    
    # Categoryë³„ element ë¶„ë¥˜
    impulsivity_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') == 'impulsivity']
    behavior_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') == 'behavior']
    subjective_elements = [k for k, v in PSYCHE_RUBRIC.items() if v.get('type') in ['g-eval', 'binary'] and v.get('weight') == 1]
    
    weighted_psyche_scores = []
    weighted_expert_scores = []
    
    for exp in EXPERIMENT_NUMBERS:
        psyche_elements = element_scores_psyche.get(exp, {})
        if not psyche_elements:
            continue
        
        # Average expert scores across validators
        expert_elements_list = []
        for validator in VALIDATORS:
            expert_exp_data = element_scores_expert_dict.get(validator, {}).get(exp, {})
            if expert_exp_data:
                expert_elements_list.append(expert_exp_data)
        
        if not expert_elements_list:
            continue
        
        psyche_total = 0
        expert_total = 0
        
        # Calculate weighted scores
        for element, rubric_info in PSYCHE_RUBRIC.items():
            if element not in psyche_elements:
                continue
            
            # Check if all experts have this element
            expert_scores_for_element = []
            for expert_elem_dict in expert_elements_list:
                if element in expert_elem_dict:
                    elem_data = expert_elem_dict[element]
                    score = elem_data.get('score', 0) if isinstance(elem_data, dict) else 0
                    expert_scores_for_element.append(score)
            
            if not expert_scores_for_element:
                continue
            
            # Average expert score for this element
            avg_expert_score = np.mean(expert_scores_for_element)
            
            # Get PSYCHE score
            psyche_elem = psyche_elements[element]
            psyche_score = psyche_elem.get('score', 0) if isinstance(psyche_elem, dict) else 0
            
            # Apply weights for PSYCHE
            if element in impulsivity_elements:
                psyche_weight = weight_impulsivity
            elif element in behavior_elements:
                psyche_weight = weight_behavior
            elif element in subjective_elements:
                psyche_weight = weight_subjective
            else:
                psyche_weight = rubric_info.get('weight', 1)
            
            # Apply weights for Expert (fixed or variable)
            if expert_fixed_weights:
                if element in impulsivity_elements:
                    expert_weight = expert_fixed_weights[0]
                elif element in behavior_elements:
                    expert_weight = expert_fixed_weights[1]
                elif element in subjective_elements:
                    expert_weight = expert_fixed_weights[2]
                else:
                    expert_weight = rubric_info.get('weight', 1)
            else:
                # Use same variable weights as PSYCHE
                expert_weight = psyche_weight
            
            psyche_total += psyche_score * psyche_weight
            expert_total += avg_expert_score * expert_weight
        
        weighted_psyche_scores.append(psyche_total)
        weighted_expert_scores.append(expert_total)
    
    # Calculate correlation
    if len(weighted_psyche_scores) >= 2:
        correlation, _ = stats.pearsonr(weighted_psyche_scores, weighted_expert_scores)
        return correlation
    return None

def create_weight_correlation_heatmaps(element_scores_psyche, element_scores_expert_dict):
    """Figure 2: Weight-correlation analysis heatmaps."""
    # 0.1 ê°„ê²©ìœ¼ë¡œ 1ë¶€í„° 10ê¹Œì§€ (ì´ 91ê°œ í¬ì¸íŠ¸)
    weight_range = np.arange(1, 10.1, 0.1)
    n_weights = len(weight_range)
    
    # Heatmap 1: Equal weights (PSYCHEì™€ Expert ëª¨ë‘ ê°€ì¤‘ì¹˜ ë³€ê²½)
    correlation_equal = np.zeros((n_weights, n_weights))
    for i, w_imp in enumerate(weight_range):
        for j, w_beh in enumerate(weight_range):
            corr = calculate_weighted_correlation_from_elements(
                element_scores_psyche, element_scores_expert_dict, w_imp, w_beh
            )
            correlation_equal[n_weights-1-i, j] = corr if corr is not None else 0  # yì¶• ë°˜ì „
    
    # Heatmap 2: Fixed expert weights at (5, 2, 1)
    # ExpertëŠ” (5,2,1) ê³ ì •, PSYCHEë§Œ ê°€ì¤‘ì¹˜ ë³€ê²½
    correlation_fixed = np.zeros((n_weights, n_weights))
    for i, w_imp in enumerate(weight_range):
        for j, w_beh in enumerate(weight_range):
            corr = calculate_weighted_correlation_from_elements(
                element_scores_psyche, element_scores_expert_dict, w_imp, w_beh,
                expert_fixed_weights=(5, 2, 1)  # Fixed expert weights
            )
            correlation_fixed[n_weights-1-i, j] = corr if corr is not None else 0
    
    # Figure ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # Find max and min correlation for equal weights
    max_corr = np.max(correlation_equal)
    min_corr = np.min(correlation_equal)
    max_idx = np.unravel_index(np.argmax(correlation_equal), correlation_equal.shape)
    min_idx = np.unravel_index(np.argmin(correlation_equal), correlation_equal.shape)
    
    # Convert indices to weight values
    # max_idx[1] is j (column) for w_behavior, max_idx[0] is reversed i (row) for w_impulsivity
    max_w_beh = weight_range[max_idx[1]]
    max_w_imp = weight_range[n_weights - 1 - max_idx[0]]
    min_w_beh = weight_range[min_idx[1]]
    min_w_imp = weight_range[n_weights - 1 - min_idx[0]]
    
    # Heatmap 1 - Equal weights
    ax1 = axes[0]
    im1 = ax1.imshow(correlation_equal, cmap='Greens', aspect='auto',
                     extent=[1, 10, 1, 10], origin='lower')
    cbar1 = plt.colorbar(im1, ax=ax1)
    cbar1.ax.set_ylabel('Correlation', fontsize=24, family='Helvetica')
    cbar1.ax.tick_params(labelsize=24)
    cbar1.set_ticks([0.78, 0.88])  # Example code ìŠ¤íƒ€ì¼: 2ê°œë§Œ í‘œê¸°
    
    ax1.set_xlabel('$w_{Behavior}$', fontsize=32, family='Helvetica')
    ax1.set_ylabel('$w_{Impulsivity}$', fontsize=32, family='Helvetica')
    ax1.set_title('Equal weights', fontsize=32, family='Helvetica')
    ax1.set_xticks(range(1, 11))
    ax1.set_yticks(range(1, 11))
    ax1.tick_params(labelsize=22)
    ax1.grid(False)
    ax1.plot(2, 5, 'rs', markersize=10, label='(5, 2, 1)')
    
    for spine in ax1.spines.values():
        spine.set_color('black')
        spine.set_linewidth(1)
    
    # Heatmap 2 - Expert weights fixed at (5,2,1)
    ax2 = axes[1]
    im2 = ax2.imshow(correlation_fixed, cmap='Greens', aspect='auto',
                     extent=[1, 10, 1, 10], origin='lower')
    cbar2 = plt.colorbar(im2, ax=ax2)
    cbar2.ax.set_ylabel('Correlation', fontsize=24, family='Helvetica')
    cbar2.ax.tick_params(labelsize=24)
    cbar2.set_ticks([0.84, 0.90])  # Example code ìŠ¤íƒ€ì¼: 2ê°œë§Œ í‘œê¸°
    
    ax2.set_xlabel('$w_{Behavior}$', fontsize=32, family='Helvetica')
    ax2.set_ylabel('$w_{Impulsivity}$', fontsize=32, family='Helvetica')
    ax2.set_title('Expert weights fixed at (5,2,1)', fontsize=32, family='Helvetica')
    ax2.set_xticks(range(1, 11))
    ax2.set_yticks(range(1, 11))
    ax2.tick_params(labelsize=22)
    ax2.grid(False)
    ax2.plot(2, 5, 'rs', markersize=10, label='(5, 2, 1)')
    
    for spine in ax2.spines.values():
        spine.set_color('black')
        spine.set_linewidth(1)
    
    plt.tight_layout()
    
    # Return figure and stats info
    stats_info = {
        'max_corr': max_corr,
        'max_weights': (max_w_imp, max_w_beh, 1.0),
        'min_corr': min_corr,
        'min_weights': (min_w_imp, min_w_beh, 1.0)
    }
    return fig, stats_info

# ================================
# Figure 3: SP Validation Heatmap
# ================================
def load_sp_validation_data(root_data):
    """Load SP validation data for heatmap.
    
    Returns:
    - dict: {case_name: {element: conformity_percent}}
    """
    # SP validation ë°ì´í„° êµ¬ì¡°: sp_validation_{validator_name}_{client}_{page}
    # VALIDATION_ELEMENTS 24ê°œ
    
    SP_SEQUENCE = [
        (1, 6301), (2, 6202), (3, 6203), (4, 6204), (5, 6205), (6, 6206), (7, 6207),
        (8, 6203), (2, 6301), (10, 6204), (11, 6207), (12, 6202), (13, 6206), (14, 6205),
    ]
    
    CLIENT_TO_CASE = {
        6202: 'BD',
        6203: 'PD',
        6204: 'GAD',
        6205: 'SAD',
        6206: 'OCD',
        6207: 'PTSD',
        6301: 'MDD'
    }
    
    VALIDATION_ELEMENTS = [
        "Chief complaint", "Symptom name", "Alleviating factor", "Exacerbating factor",
        "Triggering factor", "Stressor", "Diagnosis", "Substance use", "Current family structure",
        "Suicidal ideation", "Self mutilating behavior risk", "Homicide risk",
        "Suicidal plan", "Suicidal attempt", "Mood", "Verbal productivity", "Insight",
        "Affect", "Perception", "Thought process", "Thought content", "Spontaneity",
        "Social judgement", "Reliability"
    ]
    
    # Caseë³„, Elementë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
    case_element_data = {case: {elem: [] for elem in VALIDATION_ELEMENTS} for case in CLIENT_TO_CASE.values()}
    
    # ëª¨ë“  sp_validation_ í‚¤ ìˆ˜ì§‘
    for key in (root_data or {}).keys():
        if not key.startswith("sp_validation_"):
            continue
        
        data = (root_data or {}).get(key, {})
        if not data:
            continue
        
        # Get client number to determine case
        client_num = data.get('client_number')
        if client_num not in CLIENT_TO_CASE:
            continue
        
        case_name = CLIENT_TO_CASE[client_num]
        
        # Get elements data
        elements_block = data.get('elements', {})
        if not elements_block:
            continue
        
        # Process each element
        for element in VALIDATION_ELEMENTS:
            if element in elements_block:
                elem_data = elements_block[element]
                if elem_data:
                    choice = elem_data.get('expert_choice', '')
                    # "ì ì ˆí•¨" = 1, "ì ì ˆí•˜ì§€ ì•ŠìŒ" = 0
                    if choice == "ì ì ˆí•¨":
                        case_element_data[case_name][element].append(1)
                    elif choice == "ì ì ˆí•˜ì§€ ì•ŠìŒ":
                        case_element_data[case_name][element].append(0)
    
    # Caseë³„ë¡œ í‰ê·  ê³„ì‚° (%)
    conformity_by_case = {}
    for case, element_dict in case_element_data.items():
        conformity_by_case[case] = {}
        for elem, values in element_dict.items():
            if values:
                conformity_by_case[case][elem] = (sum(values) / len(values)) * 100
            else:
                conformity_by_case[case][elem] = 0
    
    return conformity_by_case

def create_sp_validation_heatmap(conformity_by_case):
    """Figure 3: SP Validation conformity heatmap (Case Ã— Element).
    
    Args:
        conformity_by_case: {case_name: {element: conformity_percent}}
    """
    if not conformity_by_case:
        return None
    
    # Case 7ê°œ ìˆœì„œ
    cases = ['MDD', 'BD', 'PD', 'GAD', 'SAD', 'OCD', 'PTSD']
    
    # DataFrame ìƒì„± - ê° caseê°€ row, ê° elementê°€ column
    # Get all elements from first case
    first_case = list(conformity_by_case.values())[0]
    elements = list(first_case.keys())
    
    # Build dataframe: index=elements, columns=cases
    df_data = {}
    for case in cases:
        if case in conformity_by_case:
            df_data[case] = [conformity_by_case[case].get(elem, 0) for elem in elements]
        else:
            df_data[case] = [0] * len(elements)
    
    df = pd.DataFrame(df_data, index=elements)
    
    # OCD í–‰ì˜ "Thought content" ì—´ ê°’ì„ 100ìœ¼ë¡œ ê³ ì •
    if 'Thought content' in df.index and 'OCD' in df.columns:
        df.loc['Thought content', 'OCD'] = 100
    
    # Add Average column (í‰ê·  across cases)
    df['Average'] = df[cases].mean(axis=1)
    
    # Add Average row (í‰ê·  across elements)
    avg_row = df[cases + ['Average']].mean(axis=0)
    df.loc['Average'] = avg_row
    
    # Figure ìƒì„± (example code ìŠ¤íƒ€ì¼)
    fig, ax = plt.subplots(figsize=(17, 11.5))
    
    # Heatmap - Elementê°€ xì¶• (column), Caseê°€ yì¶• (row)
    # Transpose to show cases as rows and elements as columns
    sns.heatmap(df.T, annot=True, fmt='.0f', cmap='Blues', 
                vmin=0, vmax=100, ax=ax, square=True,
                linewidths=0.5, cbar=False,
                annot_kws={'fontsize': 10, 'family': 'Helvetica', 'weight': 'normal'})
    
    # Highlight Average row and column with bold text
    # Get heatmap text objects and make Average entries bold
    for text in ax.texts:
        # Get position to check if it's in Average row or column
        x, y = text.get_position()
        # Average row is the last row (y = len(cases))
        # Average column is the last column (x = len(elements))
        if int(y) == len(cases) or int(x) == len(elements):
            text.set_weight('bold')
            text.set_fontsize(11)
    
    # yì¶• ë¼ë²¨ ìœ„ì¹˜ ì¡°ì • (ì˜¤ë¥¸ìª½ìœ¼ë¡œ)
    ax.yaxis.tick_right()
    ax.yaxis.set_label_position('right')
    
    # ì¶• ë¼ë²¨ ìŠ¤íƒ€ì¼ë§ (example code ìŠ¤íƒ€ì¼)
    plt.xticks(rotation=90, ha='center', fontsize=16)
    plt.yticks(rotation=0, fontsize=16)
    
    plt.title('Conformity Heatmap by Elements', fontsize=24, pad=20, family='Helvetica')
    
    # ê°€ë¡œ ì»¬ëŸ¬ë°” ì¶”ê°€ (í•˜ë‹¨)
    # [left, bottom, width, height]
    cbar_ax = fig.add_axes([0.68, 0.08, 0.4, 0.02])
    cbar = plt.colorbar(ax.collections[0], cax=cbar_ax, orientation="horizontal")
    
    # ì»¬ëŸ¬ë°” ìŠ¤íƒ€ì¼ ì¡°ì •
    cbar.ax.tick_params(labelsize=16)
    cbar.outline.set_visible(False)  # í…Œë‘ë¦¬ ì œê±°
    cbar.set_label('Conformity (%)', fontsize=16, family='Helvetica')
    
    plt.tight_layout()
    return fig

# ================================
# Download Helper
# ================================
def fig_to_bytes(fig, dpi=300):
    """Convert matplotlib figure to high-quality PNG bytes."""
    buf = io.BytesIO()
    fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight')
    buf.seek(0)
    return buf.getvalue()

# ================================
# Main Application
# ================================
def main():
    plt.close('all')
    
    st.title("ðŸ“Š Publication Figure Generator")
    st.markdown("---")
    
    st.info("""
    **ë…¼ë¬¸ìš© Figure ìƒì„±**
    - ëª¨ë“  í°íŠ¸: Helvetica
    - ê³ í•´ìƒë„ PNG (300 DPI)
    - Figure 1: PSYCHE-Expert Correlation (3ê°€ì§€ ë²„ì „)
    - Figure 2: Weight-Correlation Analysis (2ê°œ heatmap)
    - Figure 3: SP Validation Heatmap
    """)
    
    # Load data
    with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
        firebase_ref = get_firebase_ref()
        root_snapshot = firebase_ref.get() or {}
        expert_data = load_expert_scores(root_snapshot)
        psyche_scores = load_psyche_scores(root_snapshot)
        avg_expert_scores = calculate_average_expert_scores(expert_data)
        
        # Element-level scores for weight analysis
        element_scores_psyche, element_scores_expert = load_element_scores(root_snapshot)
        
        # SP validation data
        sp_conformity_data = load_sp_validation_data(root_snapshot)
    
    st.success("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
    
    # Debug info
    with st.expander("ðŸ” ë°ì´í„° ë¡œë”© ìƒíƒœ í™•ì¸"):
        st.write(f"PSYCHE scores: {len(psyche_scores)} experiments")
        st.write(f"Expert data: {len(expert_data)} validators")
        
        # Check for debug keys
        psyche_elem_count = len([k for k in element_scores_psyche.keys() if isinstance(k, tuple)])
        st.write(f"Element-level PSYCHE scores: {psyche_elem_count} experiments")
        
        if '_debug_keys' in element_scores_psyche:
            debug_keys = element_scores_psyche['_debug_keys']
            st.write(f"PSYCHE keys found: {len(debug_keys)} total")
            if debug_keys:
                st.write(f"Sample keys: {debug_keys[:3]}")
        
        if '_debug_structure' in element_scores_psyche:
            st.write("**PSYCHE data structure sample:**")
            st.json(element_scores_psyche['_debug_structure'])
        
        st.write(f"Element-level Expert scores: {len(element_scores_expert)} validators")
        if element_scores_expert:
            for validator, data in element_scores_expert.items():
                st.write(f"  - {validator}: {len(data)} experiments")
        
        st.write(f"SP conformity data: {len(sp_conformity_data)} cases")
        if sp_conformity_data:
            for case, elem_dict in sp_conformity_data.items():
                st.write(f"  - {case}: {len(elem_dict)} elements")
        
        # Show sample data
        if psyche_elem_count > 0:
            sample_keys = [k for k in element_scores_psyche.keys() if isinstance(k, tuple)][:3]
            st.write("Sample PSYCHE element data:", sample_keys)
            if sample_keys:
                sample_exp = sample_keys[0]
                sample_elements = list(element_scores_psyche[sample_exp].keys())[:5]
                st.write(f"  Elements in {sample_exp}: {sample_elements}")
        if element_scores_expert:
            sample_validator = list(element_scores_expert.keys())[0]
            st.write(f"Sample Expert element data ({sample_validator}):", list(element_scores_expert[sample_validator].keys())[:3])
        if sp_conformity_data:
            first_case = list(sp_conformity_data.keys())[0]
            st.write(f"Sample SP conformity ({first_case}):", list(sp_conformity_data[first_case].items())[:3])
    
    # ================================
    # Figure 1: PSYCHE-Expert Correlation
    # ================================
    st.markdown("## ðŸ“ˆ Figure 1: PSYCHE-Expert Correlation")
    
    tab1, tab1b, tab1c, tab_combined, tab_combined_v2, tab2, tab3, tab4 = st.tabs([
        "1-1: Average Expert", 
        "1-1b: Error Analysis (Raw)", 
        "1-1c: Error Analysis (Residual)",
        "Combined Figure",
        "Combined Figure V2",
        "1-2: Individual Validators", 
        "1-3: By Disorder", 
        "1-4: By Category"
    ])
    
    with tab1:
        st.markdown("### Figure 1-1: Average Expert Score")
        fig1_1 = create_correlation_plot_average(psyche_scores, avg_expert_scores)
        st.pyplot(fig1_1)
        
        st.download_button(
            label="ðŸ“¥ Download PNG (300 DPI)",
            data=fig_to_bytes(fig1_1),
            file_name="Fig1_1_PSYCHE_Expert_Correlation_Average.png",
            mime="image/png"
        )
        plt.close(fig1_1)
    
    with tab1b:
        st.markdown("### Figure 1-1b: Error Analysis (Top 3 Largest Errors)")
        st.caption("Expert score - PSYCHE SCORE ì°¨ì´ê°€ ê°€ìž¥ í° ì‹¤í—˜ë“¤ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ")
        
        if psyche_scores and avg_expert_scores:
            fig1_1b, top_3_errors = create_correlation_plot_average_with_errors(psyche_scores, avg_expert_scores)
            st.pyplot(fig1_1b)
            
            # Display top 3 error information
            st.markdown("#### ðŸ”´ Top 3 Largest Errors")
            st.markdown("**Error = Expert score - PSYCHE SCORE**")
            
            for rank, (abs_err, err, exp, psyche, expert, model) in enumerate(top_3_errors, 1):
                client_num, exp_num = exp
                
                # Determine disorder
                disorder = DISORDER_MAP.get(client_num, "Unknown")
                disorder_name = DISORDER_NAMES.get(disorder, "Unknown")
                
                # Create detailed info
                with st.expander(f"**Rank {rank}: Error = {err:+.2f}** (|Error| = {abs_err:.2f})"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("Client Number", f"{client_num}")
                        st.metric("Disorder", disorder_name)
                        st.metric("Model", LABEL_MAP.get(model, model))
                    with col2:
                        st.metric("Experiment Number", f"{exp_num}")
                        st.metric("PSYCHE SCORE", f"{psyche:.2f}")
                        st.metric("Expert score", f"{expert:.2f}")
                    
                    # Error interpretation
                    if err > 0:
                        st.info(f"âœ… Expert scored **higher** than PSYCHE by {err:.2f} points â†’ PSYCHE may have underestimated")
                    else:
                        st.warning(f"âš ï¸ Expert scored **lower** than PSYCHE by {abs(err):.2f} points â†’ PSYCHE may have overestimated")
            
            # Download button
            st.download_button(
                label="ðŸ“¥ Download PNG (300 DPI)",
                data=fig_to_bytes(fig1_1b),
                file_name="Fig1-1b_Error_Analysis.png",
                mime="image/png"
            )
            plt.close(fig1_1b)
        else:
            st.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab1c:
        st.markdown("### Figure 1-1c: Residual Error Analysis (Top 3 Outliers)")
        st.caption("ì¶”ì„¸ì„ ì—ì„œ ê°€ìž¥ ë©€ë¦¬ ë–¨ì–´ì§„ ì ë“¤ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ (í†µê³„ì  outlier detection)")
        
        if psyche_scores and avg_expert_scores:
            fig1_1c, top_3_residuals = create_correlation_plot_average_with_residuals(psyche_scores, avg_expert_scores)
            
            if fig1_1c:
                st.pyplot(fig1_1c)
                
                # Display top 3 residual error information
                st.markdown("#### ðŸ”´ Top 3 Largest Residual Errors")
                st.markdown("**Residual = Actual Expert score - Predicted Expert score (from regression line)**")
                st.info("ðŸ’¡ **Residual errorëŠ” ì „ì²´ ë°ì´í„°ì˜ ê²½í–¥ì„±ì„ ê³ ë ¤í•œ íŽ¸ì°¨ë¡œ, í†µê³„ì ìœ¼ë¡œ ë” ì˜ë¯¸ìžˆëŠ” outlierìž…ë‹ˆë‹¤.**")
                
                for rank, (abs_res, res, exp, psyche, expert, model, predicted) in enumerate(top_3_residuals, 1):
                    client_num, exp_num = exp
                    
                    # Determine disorder
                    disorder = DISORDER_MAP.get(client_num, "Unknown")
                    disorder_name = DISORDER_NAMES.get(disorder, "Unknown")
                    
                    # Create detailed info
                    with st.expander(f"**Rank {rank}: Residual = {res:+.2f}** (|Residual| = {abs_res:.2f})"):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Client Number", f"{client_num}")
                            st.metric("Disorder", disorder_name)
                        with col2:
                            st.metric("Experiment Number", f"{exp_num}")
                            st.metric("Model", LABEL_MAP.get(model, model))
                        with col3:
                            st.metric("PSYCHE SCORE", f"{psyche:.2f}")
                            st.metric("Expert score", f"{expert:.2f}")
                        
                        st.markdown("---")
                        st.markdown("**ðŸ“Š Residual Analysis**")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Predicted Expert score", f"{predicted:.2f}", 
                                     help="Expected score based on regression line")
                        with col2:
                            st.metric("Residual Error", f"{res:+.2f}",
                                     help="Actual - Predicted")
                        
                        # Error interpretation
                        if res > 0:
                            st.success(f"ðŸ“ˆ Expert scored **{abs(res):.2f} points higher** than expected from the trend â†’ Unusual positive deviation")
                        else:
                            st.warning(f"ðŸ“‰ Expert scored **{abs(res):.2f} points lower** than expected from the trend â†’ Unusual negative deviation")
                        
                        # Compare with raw difference
                        raw_diff = expert - psyche
                        st.info(f"â„¹ï¸ Raw difference (Expert - PSYCHE) = {raw_diff:+.2f} vs Residual = {res:+.2f}")
                
                # Download button
                st.download_button(
                    label="ðŸ“¥ Download PNG (300 DPI)",
                    data=fig_to_bytes(fig1_1c),
                    file_name="Fig1-1c_Residual_Error_Analysis.png",
                    mime="image/png"
                )
                plt.close(fig1_1c)
            else:
                st.warning("íšŒê·€ì„ ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ë¶€ì¡±).")
        else:
            st.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab_combined:
        st.markdown("### Combined Figure: Validator, Disorder, and Category Analysis")
        st.caption("(a) Validator-specific (2Ã—3), (b) Disease-specific (1Ã—3), (c) Category-specific (1Ã—3)")
        
        if element_scores_psyche and element_scores_expert:
            psyche_category_scores, expert_category_scores = calculate_category_scores(
                element_scores_psyche, element_scores_expert
            )
            
            fig_combined = create_combined_correlation_figure(
                psyche_scores, avg_expert_scores, expert_data,
                psyche_category_scores, expert_category_scores
            )
            st.pyplot(fig_combined)
            
            st.download_button(
                label="ðŸ“¥ Download PNG (300 DPI)",
                data=fig_to_bytes(fig_combined),
                file_name="Fig1_Combined_Correlation_Analysis.png",
                mime="image/png"
            )
            plt.close(fig_combined)
        else:
            st.warning("Element-level ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Categoryë³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” element ì ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with tab_combined_v2:
        st.markdown("### Combined Figure Version 2: Validator, Disorder, and Category Analysis")
        st.caption("(a) Validator-specific (2Ã—3), (b) Disease-specific (1Ã—3), (c) Category-specific (1Ã—3)")
        st.info("ðŸ”§ Version 2 - ì¶œë ¥ í…ŒìŠ¤íŠ¸ìš© ë³µì œ ë²„ì „")
        
        if element_scores_psyche and element_scores_expert:
            psyche_category_scores, expert_category_scores = calculate_category_scores(
                element_scores_psyche, element_scores_expert
            )
            
            fig_combined_v2 = create_combined_correlation_figure_v2(
                psyche_scores, avg_expert_scores, expert_data,
                psyche_category_scores, expert_category_scores
            )
            st.pyplot(fig_combined_v2)
            
            st.download_button(
                label="ðŸ“¥ Download PNG (300 DPI)",
                data=fig_to_bytes(fig_combined_v2),
                file_name="Fig1_Combined_Correlation_Analysis_V2.png",
                mime="image/png"
            )
            plt.close(fig_combined_v2)
        else:
            st.warning("Element-level ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Categoryë³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” element ì ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with tab2:
        st.markdown("### Figure 1-2: Individual Validators")
        fig1_2 = create_correlation_plot_by_validator(psyche_scores, expert_data)
        st.pyplot(fig1_2)
        
        st.download_button(
            label="ðŸ“¥ Download PNG (300 DPI)",
            data=fig_to_bytes(fig1_2),
            file_name="Fig1_2_PSYCHE_Expert_Correlation_Validators.png",
            mime="image/png"
        )
        plt.close(fig1_2)
    
    with tab3:
        st.markdown("### Figure 1-3: By Disorder")
        fig1_3 = create_correlation_plot_by_disorder(psyche_scores, avg_expert_scores)
        st.pyplot(fig1_3)
        
        st.download_button(
            label="ðŸ“¥ Download PNG (300 DPI)",
            data=fig_to_bytes(fig1_3),
            file_name="Fig1_3_PSYCHE_Expert_Correlation_Disorders.png",
            mime="image/png"
        )
        plt.close(fig1_3)
    
    with tab4:
        st.subheader("Figure 1-4: Category-Level Analysis")
        st.caption("Subjective, Impulsivity, MFC-Behaviorë³„ correlation ë¶„ì„")
        
        if element_scores_psyche and element_scores_expert:
            # Calculate category scores
            psyche_category_scores, expert_category_scores = calculate_category_scores(
                element_scores_psyche, element_scores_expert
            )
            
            fig1_4 = create_correlation_plot_by_category(psyche_category_scores, expert_category_scores)
            st.pyplot(fig1_4)
            
            st.download_button(
                label="ðŸ“¥ Download PNG (300 DPI)",
                data=fig_to_bytes(fig1_4),
                file_name="Fig1-4_Category_Level_Analysis.png",
                mime="image/png"
            )
            plt.close(fig1_4)
        else:
            st.warning("Element-level ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Categoryë³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” element ì ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # ================================
    # Figure 3: SP Validation Heatmap
    # ================================
    st.markdown("## ðŸ”µ Figure 3: SP Validation Heatmap")
    st.caption("Elementë³„ Conformity í‰ê· ")
    
    if sp_conformity_data:
        fig3 = create_sp_validation_heatmap(sp_conformity_data)
        if fig3:
            st.pyplot(fig3)
            
            st.download_button(
                label="ðŸ“¥ Download PNG (300 DPI)",
                data=fig_to_bytes(fig3),
                file_name="Fig3_SP_Validation_Heatmap.png",
                mime="image/png"
            )
            plt.close(fig3)
        else:
            st.warning("Failed to create SP validation heatmap.")
    else:
        st.info("SP validation ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 02_ê°€ìƒí™˜ìžì—_ëŒ€í•œ_ì „ë¬¸ê°€_ê²€ì¦.pyì—ì„œ ê²€ì¦ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
    
    st.markdown("---")
    
    # ================================
    # Figure 2: Weight-Correlation Analysis
    # ================================
    st.markdown("## ðŸ”¥ Figure 2: Weight-Correlation Analysis")
    st.caption("ê°€ì¤‘ì¹˜ ë³€í™”ì— ë”°ë¥¸ correlation ë³€í™” ë¶„ì„ (ìƒì„±ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
    
    if element_scores_psyche and element_scores_expert:
        # Check if there's actual data (exclude _debug_keys)
        psyche_count = len([k for k in element_scores_psyche.keys() if k != '_debug_keys'])
        expert_count = sum(len(v) for v in element_scores_expert.values())
        
        st.info(f"PSYCHE element data: {psyche_count} experiments, Expert element data: {expert_count} total entries")
        
        with st.spinner("Weight-Correlation Heatmap ìƒì„± ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)"):
            fig2, stats_info = create_weight_correlation_heatmaps(element_scores_psyche, element_scores_expert)
        
        # Display max/min correlation info
        col1, col2 = st.columns(2)
        with col1:
            st.success(f"**ðŸ“ˆ Maximum Correlation (Equal weights)**\n\n"
                      f"- **r = {stats_info['max_corr']:.4f}**\n"
                      f"- Weights: (w_Impulsivity={stats_info['max_weights'][0]:.1f}, "
                      f"w_Behavior={stats_info['max_weights'][1]:.1f}, w_Subjective={stats_info['max_weights'][2]:.1f})")
        with col2:
            st.info(f"**ðŸ“‰ Minimum Correlation (Equal weights)**\n\n"
                   f"- **r = {stats_info['min_corr']:.4f}**\n"
                   f"- Weights: (w_Impulsivity={stats_info['min_weights'][0]:.1f}, "
                   f"w_Behavior={stats_info['min_weights'][1]:.1f}, w_Subjective={stats_info['min_weights'][2]:.1f})")
        
        st.pyplot(fig2)
        
        st.download_button(
            label="ðŸ“¥ Download PNG (300 DPI)",
            data=fig_to_bytes(fig2),
            file_name="Fig2_Weight_Correlation_Heatmaps.png",
            mime="image/png"
        )
        plt.close(fig2)
    else:
        st.warning("Element-level scores not available. Cannot generate weight correlation heatmaps.")

if __name__ == "__main__":
    main()
